{"cells":[{"cell_type":"markdown","metadata":{"id":"CVVgbwKgUPE_"},"source":["# 이미지 사이즈 옵션 정리\n","- 1. (yolo - optional) 416 -> (efficientnet) 224 (preferred)\n","- 2. (yolo - optional) 620 -> (efficientnet) 400\n","\n","`실제 이미지셋 width, height 분석 결과 `\n","- width의 평균값 :  382\n","- height의 평균값 :  430"]},{"cell_type":"markdown","metadata":{"id":"ldq_d5uyAzry"},"source":["# 구글 드라이브 연동하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6073,"status":"ok","timestamp":1719982956117,"user":{"displayName":"금융AI 1팀","userId":"14656005272163550339"},"user_tz":-540},"id":"S2M-ztYfAyRz","outputId":"080d6060-f8c9-48f1-a55b-e57d81816ad9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","None\n"]}],"source":["# 구글 드라이브 연동하기\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 경로 및 파일 관리\n","import os\n","\n","# change directory 경로 변경하기\n","print(os.chdir('/content/drive/MyDrive'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOnZYjwoHPw5"},"outputs":[],"source":["# # yolo5 파일에서 필요한 것을 설치하기 위한 경로 설정하기\n","# os.chdir('/content/drive/MyDrive/yolo_efficientnet/yolov5')\n","\n","# # 환경 설정 파일 설치 -> [변경 내용] requests==2.31.0   pillow<10.1.0\n","# %pip install -qr requirements.txt\n","\n","# # 공유 데이터셋 파일을 가져오기 위한 roboflow\n","# %pip install -q roboflow\n","\n","# # 딥러닝 라이브러리\n","# import torch\n","\n","# # yaml : json 또는 xml 처럼 데이터 송수신을 위해 사전에 정해진 규칙을 따르는 '데이터 전송 파일' 형식\n","# import yaml\n","\n","# # 이미지 보여주는 용도\n","# from PIL import Image as Img\n","# from IPython.display import Image, display # clear_output\n","\n","# # 특정 확장자 파일 불러오기 -> glob.glob() : 특정 패턴과 일치하는 모든 경로명을 리스트로 반환\n","# import glob\n","\n","# # 학습한 베스트 모델 다운로드하기\n","# from google.colab import files"]},{"cell_type":"markdown","metadata":{"id":"syL_sGmYRcP-"},"source":["### import"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5476,"status":"ok","timestamp":1719982966460,"user":{"displayName":"금융AI 1팀","userId":"14656005272163550339"},"user_tz":-540},"id":"Hb84iH0sgy64","outputId":"7d5ef7af-5e03-457a-81a9-0f25fc1e932d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: extcolors in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from extcolors) (10.4.0)\n","Requirement already satisfied: convcolors>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from extcolors) (2.2.0)\n"]}],"source":["!pip install extcolors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEFdaVjqRfju"},"outputs":[],"source":["# 딥러닝 사용\n","import torch\n","\n","# 특정 확장자 파일 불러오기 -> glob.glob() : 특정 패턴과 일치하는 모든 경로명을 리스트로 반환\n","import glob\n","\n","# 이미지 관련\n","from PIL import Image as Img\n","from PIL import ImageFile\n","import cv2\n","import numpy as np\n","import glob\n","from IPython.display import Image, display\n","\n","# 문자열 처리\n","import unicodedata\n","\n","# 색 추출\n","import extcolors\n","\n","\n","# 파일 관리\n","import os\n","\n","# efficientNet_v2_s 모델을 가지고오기 위함\n","import torchvision.models as get_model\n","import torchvision\n","\n","# input 데이터를 정규화하기 위한 transform 규칙을 적용하기 위함\n","import torchvision.transforms as transforms\n","\n","# optimizer를 가지고 오기 위함.\n","import torch.optim as optim\n","\n","# pytorch에서 지원하는 다양한 계층 및 손실함수 계산을 사용하기 위함.\n","import torch.nn as nn\n","\n","from torchvision.datasets import ImageFolder\n","from torchvision import transforms # ToTensor, Resize, Compose\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset, DataLoader\n","\n","# 스케쥴러\n","from torch.optim.lr_scheduler import OneCycleLR\n","\n","# 학습한 베스트 모델 다운로드하기\n","from google.colab import files\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AkQssmrRg9bc"},"source":["### 이미지 비율을 유지하면서 resize하는 함수 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_d03c63cg82i"},"outputs":[],"source":["def maintain_proportion_and_resize_by_cv2(image_path, size=(416, 416)):\n","\n","    # 이미지 읽기(BGR)\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        raise ValueError(f\"이미지를 열 수 없습니다: {image_path}\")\n","\n","    # 이미지 형식 변경하기(BGR -> RGB)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # 높이, 너비 추출\n","    original_height, original_width = image.shape[:2]\n","\n","    # 비율 계산\n","    ratio = min(size[0] / original_width, size[1] / original_height)\n","    new_width = int(original_width * ratio)\n","    new_height = int(original_height * ratio)\n","\n","    # 이미지 리사이즈\n","    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n","\n","    # 새로운 이미지를 패딩하여 416x416 크기로 만들기\n","    new_image = np.full((size[1], size[0], 3), (255, 255, 255), dtype=np.uint8)  # 흰색 배경으로 초기화\n","    x_offset = (size[0] - new_width) // 2\n","    y_offset = (size[1] - new_height) // 2\n","    new_image[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_image\n","\n","    return new_image\n","\n","\n","def maintain_proportion_and_resize_by_pil(image, size=(416, 416)):\n","\n","    # PIL.Image 형식의 이미지를 비율을 유지하면서 지정된 크기로 리사이즈하고,\n","    # 흰색 배경으로 패딩하여 최종 이미지를 반환\n","\n","    # 원본 이미지 크기 추출\n","    original_width, original_height = image.size\n","\n","    # 비율 계산\n","    ratio = min(size[0] / original_width, size[1] / original_height)\n","    new_width = int(original_width * ratio)\n","    new_height = int(original_height * ratio)\n","\n","    # 이미지 리사이즈\n","    resized_image = image.resize((new_width, new_height), Img.ANTIALIAS)\n","\n","    # 새로운 이미지를 패딩하여 지정된 크기로 만들기\n","    new_image = Img.new(\"RGB\", size, (255, 255, 255))  # 흰색 배경으로 초기화\n","    x_offset = (size[0] - new_width) // 2\n","    y_offset = (size[1] - new_height) // 2\n","    new_image.paste(resized_image, (x_offset, y_offset))\n","\n","    return new_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahcmHQiMs-R2"},"outputs":[],"source":["device = torch.device('cuda'if torch.cuda.is_available else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"16xXtIR0VhYc"},"source":["### 학습된 yolo 모델 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRvCz0M4eNoR"},"outputs":[],"source":["# yolov5 pt 파일 저장 이름 정의\n","yolov5_best_pt_save_name = 'final_fashion_yolo_best'\n","\n","# yolo pt 파일 저장 경로 정의\n","yolov5_best_pt_save_path = f'/content/drive/MyDrive/yolo_efficientnet/yolo_best.pt/{yolov5_best_pt_save_name}.pt'\n","\n","# 학습 가중치를 적용한 모델 불러오기\n","yolov5_model = torch.hub.load('ultralytics/yolov5', 'custom', path= yolov5_best_pt_save_path)\n","\n","# device 설정\n","yolov5_model.to(device)\n","\n","# yolov5_model 평가 모드 선언\n","yolov5_model.eval()"]},{"cell_type":"markdown","metadata":{"id":"9I5A1XvBVmKb"},"source":["### 학습된 EfficientNet 모델 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":526,"status":"ok","timestamp":1719982984608,"user":{"displayName":"금융AI 1팀","userId":"14656005272163550339"},"user_tz":-540},"id":"NX7VPdi3hwaB","outputId":"371aade3-051e-4c0a-b078-90503f5c0559"},"outputs":[{"name":"stdout","output_type":"stream","text":["pt 적용 전 계층 구조 맞추기\n","Sequential(\n","  (0): Dropout(p=0.2, inplace=True)\n","  (1): Linear(in_features=1280, out_features=38, bias=True)\n",")\n"]}],"source":["# 학습되지 않은 초기화 efficientnetv2_s 모델 불러오기\n","effnet_v2_s_model = get_model.efficientnet_v2_s(pretrained=False)\n","\n","\n","# 학습된 pt 파일의 out_features 개수\n","num_classes = 38\n","\n","# out_features 클래스 개수 맞춰 주기\n","in_features = effnet_v2_s_model.classifier[1].in_features\n","effnet_v2_s_model.classifier[1] = nn.Linear(in_features, num_classes)\n","\n","# 반영 여부 확인\n","print('pt 적용 전 계층 구조 맞추기')\n","print(effnet_v2_s_model.classifier)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":682,"status":"ok","timestamp":1719982989210,"user":{"displayName":"금융AI 1팀","userId":"14656005272163550339"},"user_tz":-540},"id":"HZr4sl3TI7Ap","outputId":"97f2b0dd-55c6-4ead-eb01-b129c1e08cf7"},"outputs":[{"name":"stdout","output_type":"stream","text":["pt 적용 후 계층 구조 확인\n","Sequential(\n","  (0): Dropout(p=0.2, inplace=True)\n","  (1): Linear(in_features=1280, out_features=38, bias=True)\n",")\n"]}],"source":["# 이전까지 학습했던 파라미터 파일 적용하기\n","\n","eff_pt_root_path = '/content/drive/MyDrive/yolo_efficientnet/efficientnet_train.pt/'\n","eff_pt_name = f'trained_{260}_effnet.pt'\n","\n","checkpoint = torch.load(eff_pt_root_path + eff_pt_name)\n","\n","# Load the model state dictionary\n","effnet_v2_s_model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# device 설정\n","effnet_v2_s_model.to(device)\n","\n","# 평가 모드 선언\n","effnet_v2_s_model.eval()\n","\n","# 계층 적용 여부 점검 -> out_features : 38개여야 됨.\n","print('pt 적용 후 계층 구조 확인')\n","print(effnet_v2_s_model.classifier)"]},{"cell_type":"markdown","metadata":{"id":"NtTzCZ2DV6A0"},"source":["### yolo 모델 + efficientnet 모델 통합 로직"]},{"cell_type":"markdown","metadata":{"id":"vTJ3PjzM2Ume"},"source":["- 1. 하나의 이미지가 yolo에 들어간다 -> bounding_box(들)을 추출한다. [`원본 이미지`✅, `bounding box(들)`✅]\n","\n","- 2. bounding_box(들)에서 주요 옷 색상을 추출한다. [`색상 정보 추출`✅]\n","\n","- 3. bounding_box(들)에서 대분류 정보를 추출한다. [`대분류 정보`✅]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fdz3eQWLY7TS"},"outputs":[],"source":["color_ranges = {\n","    \"red\": ((128, 0, 0), (255, 127, 127)),\n","    \"green\": ((0, 128, 0), (127, 255, 127)),\n","    \"blue\": ((0, 0, 128), (127, 127, 255)),\n","    \"yellow\": ((128, 128, 0), (255, 255, 127)),\n","    \"magenta\": ((128, 0, 128), (255, 127, 255)),\n","    \"cyan\": ((0, 128, 128), (127, 255, 255)),\n","    \"orange\": ((128, 64, 0), (255, 191, 127)),\n","    \"purple\": ((64, 0, 64), (191, 127, 191)),\n","    \"pink\": ((255, 0, 127), (255, 191, 255)),\n","    \"lime\": ((0, 255, 0), (191, 255, 191)),\n","    \"brown\": ((64, 32, 0), (191, 127, 64)),\n","    \"gray\": ((128, 128, 128), (191, 191, 191)),\n","    \"black\": ((0, 0, 0), (63, 63, 63)),\n","    \"white\": ((192, 192, 192), (255, 255, 255))\n","}\n","\n","\n","def get_color_name(rgb_value):\n","    for color_name, (lower_bound, upper_bound) in color_ranges.items():\n","        # 특정 색깔 범위에 있으면\n","        if all(lower_bound[i] <= rgb_value[i] <= upper_bound[i] for i in range(3)):\n","            return color_name\n","    return \"UNKNOWN\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXEEJ_exfFWw"},"outputs":[],"source":["# 문자열 처리\n","def normalize(text) :\n","    normalized_text = unicodedata.normalize('NFD', text)\n","    return normalized_text\n","\n","# 클래스 라벨 사전 정의하기\n","classes_composition_list = ['니트','후드','맨투맨','셔츠블라우스','긴소매티셔츠','반소매티셔츠','민소매티셔츠','카라티셔츠','베스트','데님팬츠','슬랙스','트레이닝조거팬츠','숏팬츠','코튼팬츠','레깅스','와이드팬츠','후드집업','바람막이',\n","'코트','롱패딩','숏패딩','패딩베스트','블루종','레더자켓','무스탕','트러커자켓','블레이저','가디건','뽀글이후리스','사파리자켓','미니원피스','미디원피스','투피스','롱원피스', '점프수트','미니스커트','미디스커트','롱스커트']\n","\n","cls_labels = {normalize(cls) : i for i, cls in enumerate(classes_composition_list)}\n","\n","cls_labels = {v:k for k, v in cls_labels.items()}"]},{"cell_type":"markdown","metadata":{"id":"ZdOpkKznhJS3"},"source":["# 하나의 이미지 경로를 넣으면 상위, 하위 카테고리를 추출해주는 함수 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1flmxQFXmTj-"},"outputs":[],"source":["# yolo 대분류 정의\n","main_category = {'top' : '상의','bottom' : '바지','outer' : '아우터','skirt' : '스커트','dress' : '원피스'}\n","\n","# 대분류, 소분류 맵핑\n","categories = {\n","    '상의' : ['니트', '후드', '맨투맨','셔츠블라우스','긴소매티셔츠','반소매티셔츠','민소매티셔츠','카라티셔츠','베스트'],\n","    '바지' : ['데님팬츠', '슬랙스','트레이닝조거팬츠','숏팬츠','코튼팬츠','레깅스', '와이드팬츠'],\n","    '아우터' : ['후드집업', '바람막이', '코트', '롱패딩', '숏패딩', '패딩베스트', '블루종',\n","                '레더자켓', '무스탕', '트러커자켓', '블레이저', '가디건','뽀글이후리스','사파리자켓'],\n","    '원피스' : ['미니원피스', '미디원피스', '투피스', '롱원피스', '점프수트'],\n","    '스커트' : ['미니스커트', '미디스커트', '롱스커트']\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0yHkdVrh2GX"},"outputs":[],"source":["def extract_main_sub_category(image_path) :\n","  # 바운딩 박스 집합 배열\n","  detected_image_composition_list = []\n","  mains = []\n","  subs = []\n","\n","  # 깔끔한 bbox 추출 사진을 위한 준비\n","  resized_img_for_cropping = maintain_proportion_and_resize_by_cv2(test_image_path, (416, 416))\n","\n","  # yolo input : 416X416 resize 하기, 비율 유지하기\n","  resized_img = maintain_proportion_and_resize_by_cv2(test_image_path, (416, 416))\n","\n","  # 모델 투입-> 결과 도출\n","  result = yolov5_model(resized_img)\n","\n","  # bounding box 탐지 실패한 경우\n","  if not len(result.xyxy[0]) :\n","    return ['탐지 실패']\n","\n","  # 탐지 성공한 경우\n","  for i, bounding_box in enumerate(result.xyxy[0]):\n","    # 바운딩 박스 좌표 정의\n","    xmin, ymin, xmax, ymax = map(int, bounding_box[:4])\n","    # 신뢰도 정의\n","    confidence = round(bounding_box[4].item(), 2)\n","    # 탐지 클래스 정의\n","    cls = int(bounding_box[5])\n","    class_name = result.names[cls]\n","\n","    # bounding box 보여주기\n","    cropped_img = resized_img_for_cropping[ymin:ymax, xmin:xmax]\n","    cropped_img = Img.fromarray(cropped_img, 'RGB')\n","\n","    # 바운딩 박스 모으기\n","    detected_image_composition_list.append(cropped_img)\n","    mains.append(main_category[class_name])\n","\n","  for detected_image in detected_image_composition_list :\n","    resized_image = maintain_proportion_and_resize_by_pil(detected_image, (226, 226))\n","\n","    # display(resized_image)\n","\n","    resized_image = image_to_tensor(resized_image)\n","    resized_imgae = resized_image.unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        outputs = effnet_v2_s_model(resized_imgae)\n","\n","    max_value, max_index  = torch.max(outputs, 1)\n","    # [소분류 정보✅]\n","    subs.append(cls_labels[max_index.item()])\n","\n","  filtered_bbox = [detected_image_composition_list[i] for i in range(len(result.xyxy[0])) if subs[i] in categories[mains[i]]]\n","  filtered_main_sub = [f'{mains[i]}_{subs[i]}' for i in range(len(result.xyxy[0])) if subs[i] in categories[mains[i]]]\n","\n","  # 모든 것의 맵핑 결과 맞지 않으면\n","  if not len(filtered_bbox) :\n","    return ['맵핑 실패']\n","\n","  return (resized_img, filtered_bbox, filtered_main_sub)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RwkrjFWNpgE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U51AocWJNpd2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKL169stNpVu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1w8NuEXh3y40coZxd6YvZ15KWoGktOP-Z"},"executionInfo":{"elapsed":13471,"status":"ok","timestamp":1719986129945,"user":{"displayName":"금융AI 1팀","userId":"14656005272163550339"},"user_tz":-540},"id":"Rdk1pcBcmWVw","outputId":"9bb02f7e-2a6a-420c-93f3-36af2bf25027"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# 이미지를 텐서로 변환하기\n","image_to_tensor = transforms.Compose([\n","    transforms.ToTensor()])\n","\n","# 테스트 이미지 경로 정의 -> 학습 했던 내용을 기반으로 해보자!--> 결과가 괜찮게 나올수도?\n","image_dataset_to_test_path = \"/content/drive/MyDrive/yolo_efficientnet/datasets_for_unlabeled/\"\n","\n","# 탐지된 바운딩 박스를 모을 리스트\n","detected_image_composition_list = []\n","\n","for test_image in os.listdir(image_dataset_to_test_path)[1200:1230] :\n","\n","    test_image_path = image_dataset_to_test_path + test_image\n","\n","    # natural_image 만들기\n","    natural_img = cv2.imread(test_image_path)\n","    natural_img = cv2.cvtColor(natural_img, cv2.COLOR_BGR2RGB)\n","    natural_img = Img.fromarray(natural_img)\n","\n","    # # [원본 이미지✅] 화면출력\n","    # display(natural_img)\n","\n","    # yolo에 들어가기 위해서 416X416 사이즈로 비율을 고려하여 resize 하기\n","    resized_img = maintain_proportion_and_resize_by_cv2(test_image_path, (416, 416))\n","\n","    # 깔끔한 추출 사진을 얻기 위해서 정의\n","    resized_img_for_cropping = maintain_proportion_and_resize_by_cv2(test_image_path, (416, 416))\n","\n","    # resize한 사진을 모델에 넣고\n","    result = yolov5_model(resized_img)\n","\n","    # 탐지한 것이 있다면\n","    if len(result.xyxy[0]) != 0:\n","        print(f'{test_image}의 탐지 결과 :{len(result.xyxy[0])}개 ')\n","\n","    else :\n","        print(f'{test_image}의 탐지 결과 없음')\n","\n","    for i, bounding_box in enumerate(result.xyxy[0]):\n","        xmin, ymin, xmax, ymax = map(int, bounding_box[:4])\n","\n","        # [대분류 정보✅]\n","        cls = int(bounding_box[5])\n","        class_name = result.names[cls]\n","\n","        # 신뢰도 측정\n","        confidence = round(bounding_box[4].item(), 2)\n","\n","        # bounding box 보여주기\n","        cropped_img = resized_img_for_cropping[ymin:ymax, xmin:xmax]\n","        cropped_img = Img.fromarray(cropped_img, 'RGB')\n","\n","        # 바운딩 박스 모으기\n","        detected_image_composition_list.append(cropped_img)\n","\n","        # [resize된 bounding box(들)✅]\n","        display(maintain_proportion_and_resize_by_pil(cropped_img, (224, 224)))\n","\n","        print(main_category[class_name])\n","\n","    print('_________________________________________________________')\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1RZVeoPwxhzVSJoHc0xhlnbtlU3zcdSzm"},"executionInfo":{"elapsed":12983,"status":"ok","timestamp":1719988337086,"user":{"displayName":"금융AI 1팀","userId":"14656005272163550339"},"user_tz":-540},"id":"CFTwsBDIKVUl","outputId":"a533607e-2ab8-408d-b3c3-c9ea17a4e1bc"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# bounding box 모은 것을 반복문 돌리면서 resize 하기 -> (224, 224)\n","# [resize 이미지✅]\n","for detected_image in detected_image_composition_list :\n","    resized_image = maintain_proportion_and_resize_by_pil(detected_image, (224, 224))\n","\n","    display(resized_image)\n","\n","    resized_image = image_to_tensor(resized_image)\n","    resized_imgae = resized_image.unsqueeze(0).to(device)\n","\n","\n","\n","    with torch.no_grad():\n","        outputs = effnet_v2_s_model(resized_imgae)\n","\n","    max_value, max_index  = torch.max(outputs, 1)\n","    # [소분류 정보✅]\n","    print(cls_labels[max_index.item()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W8Rk336TSwSM"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}